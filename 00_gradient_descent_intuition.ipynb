{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 第零课：理解梯度下降\n",
                "\n",
                "## 前言\n",
                "\n",
                "在学习深度学习之前，你必须先理解**梯度下降 (Gradient Descent)**。\n",
                "\n",
                "这是一种用来\"找最小值\"的方法。听起来很抽象？没关系，我们用最简单的例子来理解它。\n",
                "\n",
                "---\n",
                "\n",
                "## 问题引入：如何找到函数的最低点？\n",
                "\n",
                "假设有一个函数：\n",
                "\n",
                "$$f(x) = x^2$$\n",
                "\n",
                "它的图像是一条抛物线，最低点在 $x = 0$ 处。\n",
                "\n",
                "**问题**：如果我们不知道最低点在哪，能否通过某种\"算法\"自动找到它？\n",
                "\n",
                "答案是：**可以，用梯度下降！**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']\n",
                "plt.rcParams['axes.unicode_minus'] = False"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 1. 先看一下我们要优化的函数\n",
                "\n",
                "我们选择最简单的二次函数：$f(x) = x^2$\n",
                "\n",
                "- 这个函数的最小值点在 $x = 0$\n",
                "- 最小值是 $f(0) = 0$\n",
                "\n",
                "让我们画出这个函数："
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def f(x):\n",
                "    \"\"\"我们要优化的函数\"\"\"\n",
                "    return x ** 2\n",
                "\n",
                "x_plot = np.linspace(-5, 5, 100)\n",
                "y_plot = f(x_plot)\n",
                "\n",
                "plt.figure(figsize=(8, 5))\n",
                "plt.plot(x_plot, y_plot, 'b-', linewidth=2)\n",
                "plt.scatter([0], [0], color='red', s=100, zorder=5, label='最低点 (0, 0)')\n",
                "plt.xlabel('x')\n",
                "plt.ylabel('f(x)')\n",
                "plt.title('f(x) = x²')\n",
                "plt.legend()\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 2. 什么是导数/梯度？\n",
                "\n",
                "### 直观理解：导数就是\"坡度\"\n",
                "\n",
                "想象你站在山坡上：\n",
                "- 如果脚下的坡向右上倾斜，导数为**正**（往右走会上升）\n",
                "- 如果脚下的坡向右下倾斜，导数为**负**（往右走会下降）\n",
                "- 如果脚下是平的，导数为**零**（最低点或最高点）\n",
                "\n",
                "### 数学定义\n",
                "\n",
                "对于 $f(x) = x^2$，它的导数是：\n",
                "\n",
                "$$f'(x) = 2x$$\n",
                "\n",
                "让我们验证一下：\n",
                "- 当 $x = 2$ 时，$f'(2) = 4 > 0$ → 在这点向右走会上升\n",
                "- 当 $x = -2$ 时，$f'(-2) = -4 < 0$ → 在这点向右走会下降\n",
                "- 当 $x = 0$ 时，$f'(0) = 0$ → 这就是最低点！"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def gradient(x):\n",
                "    \"\"\"f(x) = x² 的导数\"\"\"\n",
                "    return 2 * x\n",
                "\n",
                "# 在几个点上验证\n",
                "test_points = [-3, -1, 0, 1, 3]\n",
                "\n",
                "print(\"x 值   |  f(x)  | 梯度 f'(x) | 含义\")\n",
                "print(\"-\" * 55)\n",
                "for x in test_points:\n",
                "    grad = gradient(x)\n",
                "    if grad > 0:\n",
                "        meaning = \"向右上升 → 应该往左走\"\n",
                "    elif grad < 0:\n",
                "        meaning = \"向右下降 → 应该往右走\"\n",
                "    else:\n",
                "        meaning = \"平坦 → 到达最低点！\"\n",
                "    print(f\"{x:5.1f}  | {f(x):5.1f} | {grad:10.1f} | {meaning}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 3. 梯度下降的核心思想\n",
                "\n",
                "### 关键洞察\n",
                "\n",
                "- 梯度（导数）告诉我们\"上升最快\"的方向\n",
                "- 要找最小值，就要朝**梯度的反方向**走\n",
                "\n",
                "### 更新规则\n",
                "\n",
                "$$x_{new} = x_{old} - \\eta \\cdot f'(x_{old})$$\n",
                "\n",
                "其中：\n",
                "- $x_{old}$：当前位置\n",
                "- $f'(x_{old})$：当前位置的梯度\n",
                "- $\\eta$：学习率（控制每一步走多远）\n",
                "- $x_{new}$：更新后的位置\n",
                "\n",
                "### 为什么减去梯度？\n",
                "\n",
                "| 情况 | 梯度值 | 更新后 | 效果 |\n",
                "|------|--------|--------|------|\n",
                "| 在最低点右边 | 梯度 > 0 | x 减小 | 向左移动，靠近最低点 |\n",
                "| 在最低点左边 | 梯度 < 0 | x 增大 | 向右移动，靠近最低点 |\n",
                "| 在最低点 | 梯度 = 0 | x 不变 | 已到达最低点 |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 4. 手动走一遍梯度下降\n",
                "\n",
                "假设我们从 $x = 4$ 开始，学习率 $\\eta = 0.1$：\n",
                "\n",
                "| 步骤 | 当前 x | f(x) | 梯度 f'(x) | 更新计算 | 新 x |\n",
                "|------|--------|------|------------|----------|------|\n",
                "| 0 | 4.00 | 16.00 | 8.00 | 4.00 - 0.1×8.00 | 3.20 |\n",
                "| 1 | 3.20 | 10.24 | 6.40 | 3.20 - 0.1×6.40 | 2.56 |\n",
                "| 2 | 2.56 | 6.55 | 5.12 | 2.56 - 0.1×5.12 | 2.05 |\n",
                "| ... | ... | ... | ... | ... | ... |\n",
                "\n",
                "可以看到：\n",
                "1. x 在逐渐减小，向 0 靠近\n",
                "2. f(x) 也在逐渐减小\n",
                "3. 梯度越来越小（因为越接近最低点，坡度越平缓）"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 手动演示几步梯度下降\n",
                "x = 4.0\n",
                "lr = 0.1  # 学习率\n",
                "\n",
                "print(\"步骤 |   x    |  f(x)  | 梯度 f'(x) | 更新计算\")\n",
                "print(\"-\" * 60)\n",
                "\n",
                "for step in range(10):\n",
                "    fx = f(x)\n",
                "    grad = gradient(x)\n",
                "    x_new = x - lr * grad\n",
                "    print(f\" {step:2d}  | {x:6.3f} | {fx:6.3f} | {grad:10.3f} | {x:.3f} - {lr}×{grad:.3f} = {x_new:.3f}\")\n",
                "    x = x_new\n",
                "\n",
                "print(\"-\" * 60)\n",
                "print(f\"最终结果: x = {x:.6f}, f(x) = {f(x):.6f}\")\n",
                "print(f\"真实最小值: x = 0, f(x) = 0\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 5. 可视化：逐步逼近最低点\n",
                "\n",
                "下图展示了梯度下降的过程。红色的点从右边开始，一步步向最低点移动。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 记录完整的优化轨迹\n",
                "x = 4.0\n",
                "lr = 0.1\n",
                "num_steps = 20\n",
                "\n",
                "trajectory_x = [x]\n",
                "trajectory_y = [f(x)]\n",
                "\n",
                "for _ in range(num_steps):\n",
                "    grad = gradient(x)\n",
                "    x = x - lr * grad\n",
                "    trajectory_x.append(x)\n",
                "    trajectory_y.append(f(x))\n",
                "\n",
                "trajectory_x = np.array(trajectory_x)\n",
                "trajectory_y = np.array(trajectory_y)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# 左图：在函数曲线上显示优化轨迹\n",
                "ax1 = axes[0]\n",
                "ax1.plot(x_plot, y_plot, 'b-', linewidth=2, label='f(x) = x²')\n",
                "ax1.plot(trajectory_x, trajectory_y, 'ro-', markersize=8, linewidth=1.5, alpha=0.7, label='优化轨迹')\n",
                "ax1.scatter([trajectory_x[0]], [trajectory_y[0]], color='green', s=150, zorder=5, marker='s', label='起点')\n",
                "ax1.scatter([trajectory_x[-1]], [trajectory_y[-1]], color='red', s=150, zorder=5, marker='*', label='终点')\n",
                "ax1.set_xlabel('x')\n",
                "ax1.set_ylabel('f(x)')\n",
                "ax1.set_title('函数曲线上的优化轨迹')\n",
                "ax1.legend()\n",
                "ax1.grid(True, alpha=0.3)\n",
                "\n",
                "# 右图：f(x) 随步骤的变化\n",
                "ax2 = axes[1]\n",
                "ax2.plot(range(len(trajectory_y)), trajectory_y, 'g-o', linewidth=2, markersize=6)\n",
                "ax2.set_xlabel('迭代步数')\n",
                "ax2.set_ylabel('f(x)')\n",
                "ax2.set_title('f(x) 随迭代的下降过程')\n",
                "ax2.grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 6. 动画演示\n",
                "\n",
                "下面用动画更直观地展示每一步的移动过程："
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from IPython.display import HTML\n",
                "from matplotlib.animation import FuncAnimation\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(10, 6))\n",
                "\n",
                "# 画函数曲线\n",
                "ax.plot(x_plot, y_plot, 'b-', linewidth=2)\n",
                "ax.set_xlim(-5, 5)\n",
                "ax.set_ylim(-1, 20)\n",
                "ax.set_xlabel('x')\n",
                "ax.set_ylabel('f(x)')\n",
                "ax.grid(True, alpha=0.3)\n",
                "\n",
                "# 动态元素\n",
                "point, = ax.plot([], [], 'ro', markersize=15)\n",
                "trail, = ax.plot([], [], 'r--', linewidth=1, alpha=0.5)\n",
                "title = ax.set_title('')\n",
                "\n",
                "def init():\n",
                "    point.set_data([], [])\n",
                "    trail.set_data([], [])\n",
                "    return point, trail, title\n",
                "\n",
                "def animate(i):\n",
                "    point.set_data([trajectory_x[i]], [trajectory_y[i]])\n",
                "    trail.set_data(trajectory_x[:i+1], trajectory_y[:i+1])\n",
                "    title.set_text(f'步骤 {i}: x = {trajectory_x[i]:.3f}, f(x) = {trajectory_y[i]:.3f}')\n",
                "    return point, trail, title\n",
                "\n",
                "anim = FuncAnimation(fig, animate, init_func=init, frames=len(trajectory_x), interval=300, blit=False)\n",
                "plt.close()\n",
                "HTML(anim.to_jshtml())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 7. 学习率的影响\n",
                "\n",
                "学习率 $\\eta$ 决定了每一步走多远：\n",
                "\n",
                "| 学习率 | 效果 |\n",
                "|--------|------|\n",
                "| 太小 | 收敛很慢，需要很多步 |\n",
                "| 适中 | 快速且稳定地收敛 |\n",
                "| 太大 | 可能来回震荡，甚至发散 |\n",
                "\n",
                "让我们对比不同学习率的效果："
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def run_gradient_descent(x_init, lr, num_steps):\n",
                "    \"\"\"运行梯度下降，返回轨迹\"\"\"\n",
                "    x = x_init\n",
                "    traj_x = [x]\n",
                "    traj_y = [f(x)]\n",
                "    for _ in range(num_steps):\n",
                "        x = x - lr * gradient(x)\n",
                "        traj_x.append(x)\n",
                "        traj_y.append(f(x))\n",
                "    return np.array(traj_x), np.array(traj_y)\n",
                "\n",
                "learning_rates = [0.01, 0.1, 0.5, 0.95]\n",
                "colors = ['blue', 'green', 'orange', 'red']\n",
                "\n",
                "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
                "axes = axes.flatten()\n",
                "\n",
                "for idx, (lr, color) in enumerate(zip(learning_rates, colors)):\n",
                "    traj_x, traj_y = run_gradient_descent(4.0, lr, 30)\n",
                "    \n",
                "    ax = axes[idx]\n",
                "    ax.plot(x_plot, y_plot, 'gray', linewidth=1, alpha=0.5)\n",
                "    ax.plot(traj_x, traj_y, 'o-', color=color, markersize=5, linewidth=1.5)\n",
                "    ax.scatter([traj_x[0]], [traj_y[0]], color='green', s=100, zorder=5, marker='s')\n",
                "    ax.scatter([traj_x[-1]], [traj_y[-1]], color='red', s=100, zorder=5, marker='*')\n",
                "    ax.set_xlim(-6, 6)\n",
                "    ax.set_ylim(-1, 20)\n",
                "    ax.set_xlabel('x')\n",
                "    ax.set_ylabel('f(x)')\n",
                "    ax.set_title(f'学习率 η = {lr}')\n",
                "    ax.grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 对比不同学习率的收敛速度\n",
                "plt.figure(figsize=(10, 5))\n",
                "\n",
                "for lr, color in zip(learning_rates, colors):\n",
                "    _, traj_y = run_gradient_descent(4.0, lr, 30)\n",
                "    plt.plot(traj_y, 'o-', color=color, linewidth=2, markersize=4, label=f'η = {lr}')\n",
                "\n",
                "plt.xlabel('迭代步数')\n",
                "plt.ylabel('f(x)')\n",
                "plt.title('不同学习率的收敛速度对比')\n",
                "plt.legend()\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**观察结果**：\n",
                "\n",
                "- $\\eta = 0.01$：收敛非常慢，30步后还没到达最低点\n",
                "- $\\eta = 0.1$：收敛速度适中，平稳下降\n",
                "- $\\eta = 0.5$：收敛较快，但开始出现一些震荡\n",
                "- $\\eta = 0.95$：来回震荡剧烈，几乎无法收敛"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 8. 更复杂的例子：二维函数\n",
                "\n",
                "现实中我们经常要优化多个参数。让我们看一个二维的例子：\n",
                "\n",
                "$$f(x, y) = x^2 + y^2$$\n",
                "\n",
                "这个函数的最小值在 $(0, 0)$ 处。\n",
                "\n",
                "梯度变成了一个向量：\n",
                "\n",
                "$$\\nabla f = \\left( \\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y} \\right) = (2x, 2y)$$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def f_2d(x, y):\n",
                "    return x**2 + y**2\n",
                "\n",
                "def gradient_2d(x, y):\n",
                "    return 2*x, 2*y\n",
                "\n",
                "# 梯度下降\n",
                "x, y = 4.0, 3.0\n",
                "lr = 0.1\n",
                "num_steps = 30\n",
                "\n",
                "traj_x = [x]\n",
                "traj_y = [y]\n",
                "traj_f = [f_2d(x, y)]\n",
                "\n",
                "for _ in range(num_steps):\n",
                "    gx, gy = gradient_2d(x, y)\n",
                "    x = x - lr * gx\n",
                "    y = y - lr * gy\n",
                "    traj_x.append(x)\n",
                "    traj_y.append(y)\n",
                "    traj_f.append(f_2d(x, y))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 创建等高线图\n",
                "xx = np.linspace(-5, 5, 100)\n",
                "yy = np.linspace(-5, 5, 100)\n",
                "XX, YY = np.meshgrid(xx, yy)\n",
                "ZZ = f_2d(XX, YY)\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# 左图：二维等高线 + 优化轨迹\n",
                "ax1 = axes[0]\n",
                "contour = ax1.contour(XX, YY, ZZ, levels=20, cmap='viridis')\n",
                "ax1.plot(traj_x, traj_y, 'ro-', markersize=5, linewidth=1.5, label='优化轨迹')\n",
                "ax1.scatter([traj_x[0]], [traj_y[0]], color='green', s=150, zorder=5, marker='s', label='起点')\n",
                "ax1.scatter([traj_x[-1]], [traj_y[-1]], color='red', s=150, zorder=5, marker='*', label='终点')\n",
                "ax1.scatter([0], [0], color='blue', s=100, zorder=5, marker='X', label='最小值点')\n",
                "ax1.set_xlabel('x')\n",
                "ax1.set_ylabel('y')\n",
                "ax1.set_title('二维参数空间中的优化轨迹')\n",
                "ax1.legend()\n",
                "ax1.set_aspect('equal')\n",
                "\n",
                "# 右图：f 值的下降\n",
                "ax2 = axes[1]\n",
                "ax2.plot(traj_f, 'g-o', linewidth=2, markersize=5)\n",
                "ax2.set_xlabel('迭代步数')\n",
                "ax2.set_ylabel('f(x, y)')\n",
                "ax2.set_title('函数值随迭代的下降')\n",
                "ax2.grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(f\"起点: ({traj_x[0]:.3f}, {traj_y[0]:.3f}), f = {traj_f[0]:.3f}\")\n",
                "print(f\"终点: ({traj_x[-1]:.6f}, {traj_y[-1]:.6f}), f = {traj_f[-1]:.6f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 9. 链式法则：复合函数的求导\n",
                "\n",
                "在深度学习中，我们经常遇到**复合函数**——即一个函数套着另一个函数。要对这样的函数求导，就需要用到**链式法则 (Chain Rule)**。\n",
                "\n",
                "### 什么是链式法则？\n",
                "\n",
                "假设我们有两个函数：\n",
                "- 外层函数 $f(u)$\n",
                "- 内层函数 $u = g(x)$\n",
                "\n",
                "它们组合成复合函数：$y = f(g(x))$\n",
                "\n",
                "链式法则告诉我们：\n",
                "\n",
                "$$\\frac{dy}{dx} = \\frac{df}{du} \\cdot \\frac{du}{dx}$$\n",
                "\n",
                "用文字表达就是：**先对外层函数求导，再乘以内层函数的导数**。\n",
                "\n",
                "### 直观理解\n",
                "\n",
                "可以这样理解链式法则：\n",
                "- $\\frac{du}{dx}$ 表示：$x$ 变化一点点时，$u$ 变化多少\n",
                "- $\\frac{df}{du}$ 表示：$u$ 变化一点点时，$f$ 变化多少\n",
                "- 把它们乘起来，就得到：$x$ 变化一点点时，$f$ 变化多少\n",
                "\n",
                "就像多米诺骨牌：$x$ 影响 $u$，$u$ 再影响 $f$，总的影响是两步影响的**乘积**。\n",
                "\n",
                "### 具体例子\n",
                "\n",
                "假设我们要对 $y = (3x + 2)^2$ 求导。\n",
                "\n",
                "**第一步：识别内外层函数**\n",
                "- 内层函数：$u = g(x) = 3x + 2$\n",
                "- 外层函数：$f(u) = u^2$\n",
                "\n",
                "所以 $y = f(g(x)) = (3x + 2)^2$\n",
                "\n",
                "**第二步：分别求导**\n",
                "- 内层的导数：$\\frac{du}{dx} = \\frac{d(3x+2)}{dx} = 3$\n",
                "- 外层的导数：$\\frac{df}{du} = \\frac{d(u^2)}{du} = 2u$\n",
                "\n",
                "**第三步：应用链式法则**\n",
                "\n",
                "$$\\frac{dy}{dx} = \\frac{df}{du} \\cdot \\frac{du}{dx} = 2u \\cdot 3 = 6u$$\n",
                "\n",
                "**第四步：把 $u$ 换回 $x$**\n",
                "\n",
                "因为 $u = 3x + 2$，所以：\n",
                "\n",
                "$$\\frac{dy}{dx} = 6(3x + 2) = 18x + 12$$\n",
                "\n",
                "我们可以用展开法验证。把 $(3x+2)^2$ 展开：\n",
                "\n",
                "$$y = (3x+2)^2 = 9x^2 + 12x + 4$$\n",
                "\n",
                "直接求导：\n",
                "\n",
                "$$\\frac{dy}{dx} = 18x + 12$$\n",
                "\n",
                "结果一致！✓\n",
                "\n",
                "### 为什么链式法则在深度学习中很重要？\n",
                "\n",
                "神经网络本质上就是**层层嵌套的复合函数**：\n",
                "\n",
                "$$\\text{输出} = f_n(f_{n-1}(...f_2(f_1(\\text{输入}))...))$$\n",
                "\n",
                "当我们计算梯度时，需要把误差从输出层**反向传播**到每一层的参数。这个过程正是反复应用链式法则：\n",
                "\n",
                "$$\\frac{\\partial L}{\\partial w_1} = \\frac{\\partial L}{\\partial f_n} \\cdot \\frac{\\partial f_n}{\\partial f_{n-1}} \\cdot ... \\cdot \\frac{\\partial f_2}{\\partial f_1} \\cdot \\frac{\\partial f_1}{\\partial w_1}$$\n",
                "\n",
                "这就是**反向传播算法 (Backpropagation)** 的数学基础。\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 10. 总结\n",
                "\n",
                "### 核心要点\n",
                "\n",
                "1. **目标**：找到函数的最小值点\n",
                "\n",
                "2. **梯度的含义**：\n",
                "   - 梯度指向函数值**增加最快**的方向\n",
                "   - 梯度的大小表示**坡度**有多陡\n",
                "\n",
                "3. **梯度下降的原理**：\n",
                "   - 朝着梯度的**反方向**走，函数值就会下降\n",
                "   - 反复迭代，最终逼近最小值点\n",
                "\n",
                "4. **更新公式**：\n",
                "   $$x_{new} = x_{old} - \\eta \\cdot \\nabla f(x_{old})$$\n",
                "\n",
                "5. **学习率**：\n",
                "   - 太小：收敛慢\n",
                "   - 太大：震荡或发散\n",
                "   - 需要调整到合适的值\n",
                "\n",
                "### 与深度学习的联系\n",
                "\n",
                "在深度学习中：\n",
                "- $x$ 变成了模型的**所有参数**（权重和偏置）\n",
                "- $f(x)$ 变成了**损失函数**（衡量预测的误差）\n",
                "- 梯度下降用来**最小化损失函数**，从而让模型学会正确预测\n",
                "\n",
                "核心思想完全一样：**沿着梯度反方向更新参数，让损失越来越小**。\n",
                "\n",
                "---\n",
                "\n",
                "理解了这个基础，你就可以进入下一课 `01_linear_regression_from_scratch.ipynb`，学习如何用梯度下降来训练一个真正的模型！"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "heatmap",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
