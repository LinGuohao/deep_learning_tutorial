{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 深度学习第一课：一元线性回归（手写反向传播）\n",
                "\n",
                "## 为什么从这里开始？\n",
                "\n",
                "一元线性回归是所有深度学习模型的\"祖宗\"。虽然简单，但它包含了深度学习的**全部核心要素**：\n",
                "\n",
                "| 核心概念 | 在线性回归中的体现 |\n",
                "|---------|------------------|\n",
                "| 参数 | 权重 $w$ 和偏置 $b$ |\n",
                "| 损失函数 | 均方误差 MSE |\n",
                "| 梯度 | $\\frac{\\partial L}{\\partial w}$ 和 $\\frac{\\partial L}{\\partial b}$ |\n",
                "| 优化 | 梯度下降 |\n",
                "\n",
                "最重要的是：**每一步都能手算、手推、手写**。如果这一步没吃透，后面所有\"深度\"都是空的。"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 1. 环境准备"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']\n",
                "plt.rcParams['axes.unicode_minus'] = False\n",
                "np.random.seed(42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 2. 生成训练数据\n",
                "\n",
                "我们人工构造一组数据，其真实规律为：\n",
                "\n",
                "$$y = 2x + 3 + \\epsilon, \\quad \\epsilon \\sim N(0, \\sigma^2)$$\n",
                "\n",
                "其中：\n",
                "- **真实权重** $w_{true} = 2$\n",
                "- **真实偏置** $b_{true} = 3$  \n",
                "- $\\epsilon$ 是高斯噪声，模拟真实世界中的观测误差\n",
                "\n",
                "### 为什么用人工数据？\n",
                "\n",
                "因为**真值已知**，我们可以：\n",
                "1. 验证学习算法是否正确（最终参数应接近真值）\n",
                "2. 观察 Loss 是否真的在下降\n",
                "3. 通过调整噪声大小，理解\"不可约误差\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 真实参数\n",
                "w_true = 2.0\n",
                "b_true = 3.0\n",
                "\n",
                "# 生成数据\n",
                "x = np.linspace(-10, 10, 100)\n",
                "noise_std = 1.0\n",
                "y = w_true * x + b_true + np.random.randn(100) * noise_std\n",
                "\n",
                "print(f\"数据点数量: {len(x)}\")\n",
                "print(f\"真实参数: w = {w_true}, b = {b_true}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 数据可视化\n",
                "\n",
                "下图展示了生成的数据点（蓝色散点）和真实的线性函数（红色虚线）。\n",
                "\n",
                "我们的目标是：**仅从蓝色散点，推断出红色虚线的参数**。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(10, 6))\n",
                "plt.scatter(x, y, alpha=0.6, label='观测数据', color='dodgerblue', s=30)\n",
                "plt.plot(x, w_true * x + b_true, 'r--', linewidth=2, label=f'真实函数 y={w_true}x+{b_true}')\n",
                "plt.xlabel('x')\n",
                "plt.ylabel('y')\n",
                "plt.legend()\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 3. 核心公式推导\n",
                "\n",
                "### 3.1 前向传播 (Forward Pass)\n",
                "\n",
                "给定参数 $w$ 和 $b$，模型的预测值为：\n",
                "\n",
                "$$\\hat{y} = w \\cdot x + b$$\n",
                "\n",
                "### 3.2 损失函数 (Loss Function)\n",
                "\n",
                "我们使用**均方误差 (MSE)** 来衡量预测值与真实值的差距：\n",
                "\n",
                "$$L = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2$$\n",
                "\n",
                "> **核心洞察**：Loss 就是\"误差能量\"。它越小，说明我们的预测越准确。\n",
                "\n",
                "### 3.3 反向传播 (Backward Pass) — 梯度计算\n",
                "\n",
                "为了最小化 Loss，我们需要知道：**参数往哪个方向调整，Loss 会下降？**\n",
                "\n",
                "答案是**梯度的反方向**。\n",
                "\n",
                "令 $e_i = y_i - \\hat{y}_i$ (误差)，通过链式法则推导：\n",
                "\n",
                "$$\\frac{\\partial L}{\\partial w} = \\frac{\\partial}{\\partial w} \\left[ \\frac{1}{N} \\sum e_i^2 \\right] = \\frac{2}{N} \\sum e_i \\cdot \\frac{\\partial e_i}{\\partial w} = -\\frac{2}{N} \\sum x_i \\cdot e_i$$\n",
                "\n",
                "$$\\frac{\\partial L}{\\partial b} = -\\frac{2}{N} \\sum e_i$$\n",
                "\n",
                "> **核心洞察**：梯度是 Loss 增加最快的方向。我们朝梯度的**反方向**走，Loss 就会下降。\n",
                "\n",
                "### 3.4 梯度下降 (Gradient Descent)\n",
                "\n",
                "有了梯度，通过以下规则迭代更新参数：\n",
                "\n",
                "$$w \\leftarrow w - \\eta \\cdot \\frac{\\partial L}{\\partial w}$$\n",
                "$$b \\leftarrow b - \\eta \\cdot \\frac{\\partial L}{\\partial b}$$\n",
                "\n",
                "其中 $\\eta$ 是**学习率 (learning rate)**，控制每次更新的步长大小。"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 4. 代码实现\n",
                "\n",
                "下面我们将上述公式翻译成代码，完整实现一个梯度下降的线性回归。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 超参数\n",
                "learning_rate = 0.01\n",
                "num_epochs = 100\n",
                "\n",
                "# 随机初始化参数\n",
                "w = np.random.randn()\n",
                "b = np.random.randn()\n",
                "\n",
                "# 记录训练历史\n",
                "history = {'loss': [], 'w': [], 'b': []}\n",
                "N = len(x)\n",
                "\n",
                "print(f\"初始参数: w = {w:.4f}, b = {b:.4f}\")\n",
                "print(f\"目标参数: w = {w_true}, b = {b_true}\")\n",
                "print(\"-\" * 50)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for epoch in range(num_epochs):\n",
                "    # 前向传播\n",
                "    y_pred = w * x + b\n",
                "    \n",
                "    # 计算损失\n",
                "    loss = ((y - y_pred) ** 2).mean()\n",
                "    \n",
                "    # 反向传播：计算梯度\n",
                "    dw = (-2 / N) * np.sum(x * (y - y_pred))\n",
                "    db = (-2 / N) * np.sum(y - y_pred)\n",
                "    \n",
                "    # 梯度下降：更新参数\n",
                "    w = w - learning_rate * dw\n",
                "    b = b - learning_rate * db\n",
                "    \n",
                "    # 记录历史\n",
                "    history['loss'].append(loss)\n",
                "    history['w'].append(w)\n",
                "    history['b'].append(b)\n",
                "    \n",
                "    if epoch % 20 == 0 or epoch == num_epochs - 1:\n",
                "        print(f\"Epoch {epoch:3d} | Loss: {loss:.4f} | w: {w:.4f} | b: {b:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"-\" * 50)\n",
                "print(f\"最终学习到的参数: w = {w:.4f}, b = {b:.4f}\")\n",
                "print(f\"真实参数:         w = {w_true}, b = {b_true}\")\n",
                "print(f\"参数误差:         Δw = {abs(w - w_true):.4f}, Δb = {abs(b - b_true):.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 5. 训练过程可视化\n",
                "\n",
                "### 5.1 Loss 下降曲线\n",
                "\n",
                "下图展示了 Loss 随训练轮次的变化。可以观察到 Loss 逐渐下降并趋于稳定，这说明模型正在学习。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
                "\n",
                "# Loss 曲线\n",
                "axes[0].plot(history['loss'], color='crimson', linewidth=2)\n",
                "axes[0].set_xlabel('Epoch')\n",
                "axes[0].set_ylabel('Loss (MSE)')\n",
                "axes[0].set_title('Loss 下降曲线')\n",
                "axes[0].set_yscale('log')\n",
                "axes[0].grid(True, alpha=0.3)\n",
                "\n",
                "# w 的变化\n",
                "axes[1].plot(history['w'], color='forestgreen', linewidth=2, label='学习到的 w')\n",
                "axes[1].axhline(y=w_true, color='red', linestyle='--', linewidth=2, label=f'真实值 w={w_true}')\n",
                "axes[1].set_xlabel('Epoch')\n",
                "axes[1].set_ylabel('w')\n",
                "axes[1].set_title('权重 w 变化')\n",
                "axes[1].legend()\n",
                "axes[1].grid(True, alpha=0.3)\n",
                "\n",
                "# b 的变化  \n",
                "axes[2].plot(history['b'], color='purple', linewidth=2, label='学习到的 b')\n",
                "axes[2].axhline(y=b_true, color='red', linestyle='--', linewidth=2, label=f'真实值 b={b_true}')\n",
                "axes[2].set_xlabel('Epoch')\n",
                "axes[2].set_ylabel('b')\n",
                "axes[2].set_title('偏置 b 变化')\n",
                "axes[2].legend()\n",
                "axes[2].grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.2 拟合效果对比\n",
                "\n",
                "下图对比了真实函数（红色虚线）和学习到的函数（绿色实线）。可以看到两条线几乎完全重合。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(10, 6))\n",
                "plt.scatter(x, y, alpha=0.6, label='训练数据', color='dodgerblue', s=30)\n",
                "plt.plot(x, w_true * x + b_true, 'r--', linewidth=2.5, label=f'真实函数 y={w_true}x+{b_true}')\n",
                "plt.plot(x, w * x + b, 'g-', linewidth=2.5, label=f'学习结果 y={w:.2f}x+{b:.2f}')\n",
                "plt.xlabel('x')\n",
                "plt.ylabel('y')\n",
                "plt.legend()\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.3 参数空间中的优化轨迹\n",
                "\n",
                "下图展示了在参数空间 $(w, b)$ 中的优化轨迹：\n",
                "- **等高线**：表示不同参数组合对应的 Loss 值（越深色 Loss 越小）\n",
                "- **红色轨迹**：参数从初始点到最终点的移动路径\n",
                "- **黄色星**：起点\n",
                "- **绿色星**：终点\n",
                "- **红色 X**：真实参数位置\n",
                "\n",
                "这张图直观展示了梯度下降的本质：沿着 Loss 下降最快的方向，逐步逼近最优解。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 创建参数网格\n",
                "w_range = np.linspace(-1, 4, 100)\n",
                "b_range = np.linspace(-1, 6, 100)\n",
                "W, B = np.meshgrid(w_range, b_range)\n",
                "\n",
                "# 计算 Loss 曲面\n",
                "Loss_surface = np.zeros_like(W)\n",
                "for i in range(W.shape[0]):\n",
                "    for j in range(W.shape[1]):\n",
                "        y_pred_grid = W[i, j] * x + B[i, j]\n",
                "        Loss_surface[i, j] = ((y - y_pred_grid) ** 2).mean()\n",
                "\n",
                "# 绘图\n",
                "plt.figure(figsize=(10, 8))\n",
                "contour = plt.contour(W, B, Loss_surface, levels=30, cmap='viridis', alpha=0.8)\n",
                "plt.colorbar(contour, label='Loss (MSE)')\n",
                "\n",
                "plt.plot(history['w'], history['b'], 'ro-', markersize=3, linewidth=1.5, label='优化轨迹', alpha=0.8)\n",
                "plt.scatter(history['w'][0], history['b'][0], s=200, c='yellow', edgecolors='black', marker='*', zorder=5, label='起点')\n",
                "plt.scatter(history['w'][-1], history['b'][-1], s=200, c='lime', edgecolors='black', marker='*', zorder=5, label='终点')\n",
                "plt.scatter(w_true, b_true, s=200, c='red', edgecolors='white', marker='X', zorder=5, label='真实值')\n",
                "\n",
                "plt.xlabel('w')\n",
                "plt.ylabel('b')\n",
                "plt.title('参数空间中的优化轨迹')\n",
                "plt.legend(loc='upper right')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 6. 实验：动手探索\n",
                "\n",
                "以下实验帮助你更深入理解各个概念。**强烈建议亲自修改参数并运行**。\n",
                "\n",
                "### 实验 1：学习率的影响\n",
                "\n",
                "学习率 $\\eta$ 控制每次参数更新的步长：\n",
                "- **太小**：收敛很慢，需要很多轮训练\n",
                "- **适中**：收敛快且稳定\n",
                "- **太大**：可能震荡甚至发散（Loss 不降反升）\n",
                "\n",
                "下面对比三种学习率的效果："
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_with_lr(lr, num_epochs=50):\n",
                "    \"\"\"使用指定学习率训练，返回 Loss 历史\"\"\"\n",
                "    w_exp, b_exp = 0.5, 0.5  # 固定起点\n",
                "    losses = []\n",
                "    \n",
                "    for _ in range(num_epochs):\n",
                "        y_pred = w_exp * x + b_exp\n",
                "        loss = ((y - y_pred) ** 2).mean()\n",
                "        losses.append(loss)\n",
                "        \n",
                "        dw = (-2 / N) * np.sum(x * (y - y_pred))\n",
                "        db = (-2 / N) * np.sum(y - y_pred)\n",
                "        \n",
                "        w_exp = w_exp - lr * dw\n",
                "        b_exp = b_exp - lr * db\n",
                "        \n",
                "        if loss > 1e10:\n",
                "            break\n",
                "    \n",
                "    return losses\n",
                "\n",
                "learning_rates = [0.001, 0.01, 0.05]\n",
                "colors = ['blue', 'green', 'red']\n",
                "\n",
                "plt.figure(figsize=(10, 5))\n",
                "for lr, color in zip(learning_rates, colors):\n",
                "    losses = train_with_lr(lr)\n",
                "    plt.plot(losses, label=f'lr = {lr}', color=color, linewidth=2)\n",
                "\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('Loss')\n",
                "plt.title('不同学习率的 Loss 曲线对比')\n",
                "plt.legend()\n",
                "plt.yscale('log')\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 实验 2：噪声的影响\n",
                "\n",
                "噪声 $\\sigma$ 控制数据的随机性：\n",
                "- **$\\sigma = 0$**：无噪声，学习到的参数应该完全等于真值\n",
                "- **$\\sigma$ 增大**：噪声增大，学习到的参数会偏离真值\n",
                "\n",
                "**思考问题**：噪声变大后，最优解还应该等于 $(w=2, b=3)$ 吗？"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "noise_levels = [0.0, 1.0, 3.0, 5.0]\n",
                "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
                "axes = axes.flatten()\n",
                "\n",
                "for idx, noise in enumerate(noise_levels):\n",
                "    np.random.seed(42)\n",
                "    y_noisy = w_true * x + b_true + np.random.randn(100) * noise\n",
                "    \n",
                "    # 训练\n",
                "    w_n, b_n = 0.0, 0.0\n",
                "    for _ in range(200):\n",
                "        y_pred = w_n * x + b_n\n",
                "        dw = (-2 / N) * np.sum(x * (y_noisy - y_pred))\n",
                "        db = (-2 / N) * np.sum(y_noisy - y_pred)\n",
                "        w_n = w_n - 0.01 * dw\n",
                "        b_n = b_n - 0.01 * db\n",
                "    \n",
                "    # 绘图\n",
                "    axes[idx].scatter(x, y_noisy, alpha=0.5, s=20)\n",
                "    axes[idx].plot(x, w_true * x + b_true, 'r--', linewidth=2, label='真实函数')\n",
                "    axes[idx].plot(x, w_n * x + b_n, 'g-', linewidth=2, label='学习结果')\n",
                "    axes[idx].set_title(f'噪声 σ={noise}, 学习到 w={w_n:.2f}, b={b_n:.2f}')\n",
                "    axes[idx].legend()\n",
                "    axes[idx].grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**问题的答案**：\n",
                "\n",
                "理论上，当数据量足够大时，最优解的期望值仍然等于真值 $(2, 3)$，因为噪声是零均值的。\n",
                "\n",
                "但在实际中（有限样本），噪声会导致学习到的参数产生偏差。这就是所谓的**方差 (variance)** ——噪声越大，估计的不确定性越大。"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 实验 3：为什么 Loss 降了但 w 没有完全等于 2？\n",
                "\n",
                "这是一个关键问题。让我们来分析："
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_loss(w_val, b_val):\n",
                "    y_pred = w_val * x + b_val\n",
                "    return ((y - y_pred) ** 2).mean()\n",
                "\n",
                "# 真实参数的 Loss\n",
                "loss_true = compute_loss(w_true, b_true)\n",
                "\n",
                "# 学习到的参数的 Loss\n",
                "loss_learned = compute_loss(w, b)\n",
                "\n",
                "# 解析解（最小二乘法的闭式解）\n",
                "w_analytical = np.sum((x - x.mean()) * (y - y.mean())) / np.sum((x - x.mean())**2)\n",
                "b_analytical = y.mean() - w_analytical * x.mean()\n",
                "loss_analytical = compute_loss(w_analytical, b_analytical)\n",
                "\n",
                "print(\"三种参数对应的 Loss 对比：\")\n",
                "print(\"-\" * 50)\n",
                "print(f\"真实参数   (w={w_true:.4f}, b={b_true:.4f}) 的 Loss: {loss_true:.6f}\")\n",
                "print(f\"学习参数   (w={w:.4f}, b={b:.4f}) 的 Loss: {loss_learned:.6f}\")\n",
                "print(f\"解析解     (w={w_analytical:.4f}, b={b_analytical:.4f}) 的 Loss: {loss_analytical:.6f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**关键洞察**：\n",
                "\n",
                "1. **真实参数不一定是 Loss 最小的点**。因为数据有噪声，最优拟合线会略微偏向噪声。\n",
                "\n",
                "2. **解析解（最小二乘）才是 Loss 真正最小的点**。梯度下降会逼近解析解，而不是真实参数。\n",
                "\n",
                "3. **只有当噪声为 0 时，最优解才完全等于真实参数**。"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 7. 动画演示\n",
                "\n",
                "下面的动画展示了训练过程中拟合线的实时变化和 Loss 的下降过程。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from IPython.display import HTML\n",
                "from matplotlib.animation import FuncAnimation\n",
                "\n",
                "# 重新训练，保存中间状态\n",
                "np.random.seed(123)\n",
                "w_anim, b_anim = 0.0, 0.0\n",
                "lr_anim = 0.02\n",
                "frames_w, frames_b, frames_loss = [w_anim], [b_anim], []\n",
                "\n",
                "for _ in range(60):\n",
                "    y_pred = w_anim * x + b_anim\n",
                "    loss = ((y - y_pred) ** 2).mean()\n",
                "    frames_loss.append(loss)\n",
                "    \n",
                "    dw = (-2 / N) * np.sum(x * (y - y_pred))\n",
                "    db = (-2 / N) * np.sum(y - y_pred)\n",
                "    \n",
                "    w_anim = w_anim - lr_anim * dw\n",
                "    b_anim = b_anim - lr_anim * db\n",
                "    \n",
                "    frames_w.append(w_anim)\n",
                "    frames_b.append(b_anim)\n",
                "\n",
                "# 创建动画\n",
                "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "ax1.scatter(x, y, alpha=0.5, s=20)\n",
                "ax1.plot(x, w_true * x + b_true, 'r--', linewidth=2, label='真实函数')\n",
                "learned_line, = ax1.plot([], [], 'g-', linewidth=2.5, label='当前拟合')\n",
                "ax1.set_xlim(-12, 12)\n",
                "ax1.set_ylim(-25, 30)\n",
                "ax1.set_xlabel('x')\n",
                "ax1.set_ylabel('y')\n",
                "ax1.legend()\n",
                "ax1.grid(True, alpha=0.3)\n",
                "title1 = ax1.set_title('')\n",
                "\n",
                "loss_line, = ax2.plot([], [], 'crimson', linewidth=2)\n",
                "ax2.set_xlim(0, 60)\n",
                "ax2.set_ylim(0, max(frames_loss) * 1.1)\n",
                "ax2.set_xlabel('Epoch')\n",
                "ax2.set_ylabel('Loss')\n",
                "ax2.set_title('Loss 下降曲线')\n",
                "ax2.grid(True, alpha=0.3)\n",
                "\n",
                "def animate(i):\n",
                "    y_fit = frames_w[i] * x + frames_b[i]\n",
                "    learned_line.set_data(x, y_fit)\n",
                "    title1.set_text(f'Epoch {i}: w={frames_w[i]:.3f}, b={frames_b[i]:.3f}')\n",
                "    loss_line.set_data(range(i), frames_loss[:i])\n",
                "    return learned_line, title1, loss_line\n",
                "\n",
                "anim = FuncAnimation(fig, animate, frames=len(frames_w)-1, interval=100, blit=False)\n",
                "plt.close()\n",
                "HTML(anim.to_jshtml())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 8. 总结\n",
                "\n",
                "### 核心概念检查清单\n",
                "\n",
                "完成这个练习后，请确保你能回答以下问题：\n",
                "\n",
                "| 问题 | 答案要点 |\n",
                "|------|----------|\n",
                "| Loss 是什么？ | \"误差能量\"，衡量预测与真实的差距 |\n",
                "| 梯度是什么？ | Loss 相对于参数的变化率，指向 Loss 增加最快的方向 |\n",
                "| 为什么用负梯度？ | 要让 Loss 下降，必须朝梯度反方向走 |\n",
                "| 学习率的作用？ | 控制更新步长；太大震荡，太小收敛慢 |\n",
                "| 训练的本质？ | 迭代调整参数，不断减小 Loss |\n",
                "\n",
                "### 深度学习的核心循环\n",
                "\n",
                "无论模型多复杂（CNN、RNN、Transformer），核心流程永远是：\n",
                "\n",
                "```\n",
                "前向传播 → 计算损失 → 反向求梯度 → 更新参数\n",
                "```\n",
                "\n",
                "你现在已经从最简单的线性回归中，完整理解了这个过程。接下来可以挑战：\n",
                "\n",
                "1. **多元线性回归**：输入从 1 维变成多维\n",
                "2. **逻辑回归**：用于分类问题\n",
                "3. **多层感知机**：添加隐藏层和非线性激活函数"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "heatmap",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
